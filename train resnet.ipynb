{"cells":[{"cell_type":"code","execution_count":null,"id":"8d03de1e-573d-422a-8f28-c4afaa046f97","metadata":{"id":"8d03de1e-573d-422a-8f28-c4afaa046f97"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from joblib import Parallel, delayed\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","# Transforms for ResNet (224x224)\n","my_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","class MyImageDataset(Dataset):\n","    def __init__(self, parquet_path, transform=None):\n","        \"\"\"\n","        parquet_path: path to the Parquet file with columns [filepath, label].\n","        transform: optional torchvision transforms.\n","        \"\"\"\n","        # Load the Parquet file into a DataFrame\n","        self.data = pd.read_parquet(parquet_path)\n","        initial_count = len(self.data)\n","\n","        # Function to check if a file is valid (exists and > 0 bytes)\n","        def is_valid(fp):\n","            try:\n","                return os.path.exists(fp) and os.path.getsize(fp) > 0\n","            except Exception:\n","                return False\n","\n","        # Use Joblib to parallelize the file validity checks over all filepaths\n","        valid_flags = Parallel(n_jobs=-1)(\n","            delayed(is_valid)(fp) for fp in self.data['filepath']\n","        )\n","\n","        # Filter the DataFrame to keep only valid entries\n","        self.data = self.data[valid_flags].reset_index(drop=True)\n","        filtered_count = len(self.data)\n","        print(f\"Filtered dataset: kept {filtered_count} out of {initial_count} entries \"\n","              f\"(removed {initial_count - filtered_count}).\")\n","\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data.iloc[index]\n","        img_path = row['filepath']\n","        label = row['label']  # 0 or 1\n","\n","        # Open the image and convert it to RGB\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # Apply transforms if provided\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Convert label to a tensor\n","        label = torch.tensor(label, dtype=torch.long)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"id":"ddd8478c-a9c3-45f6-97f2-22cf0b076a1d","metadata":{"id":"ddd8478c-a9c3-45f6-97f2-22cf0b076a1d"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","parquet_file = 'train_data_path_test.parquet'  # The Parquet file you created\n","train_dataset = MyImageDataset(parquet_file, transform=my_transform)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=32,      # adjust based on GPU memory\n","    shuffle=True,       # randomize order of samples\n","    num_workers=8,      # use multiple CPU cores to speed up loading\n","    pin_memory=True\n",")\n","\n","\n","parquet_file = 'val_data_path_test.parquet'  # The Parquet file you created\n","val_dataset = MyImageDataset(parquet_file, transform=my_transform)\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=32,      # adjust based on GPU memory\n","    shuffle=True,       # randomize order of samples\n","    num_workers=8,      # use multiple CPU cores to speed up loading\n","    pin_memory=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"940e9290-5c75-49f4-a076-df900aa7efa2","metadata":{"scrolled":true,"id":"940e9290-5c75-49f4-a076-df900aa7efa2","outputId":"90c52702-e39d-4d2f-9c1c-70e7a797cb59"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-03-25 08:40:11.510037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742892012.152548    4168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742892012.365831    4168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-25 08:40:14.591239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/home/rayzzz0403/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/rayzzz0403/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/200 Train Loss: 0.7105 Acc: 0.5048 | Val Loss: 0.7597 Acc: 0.4985 | Time: 140.9s\n","  --> Saved checkpoint to ./checkpoints_simple/resnet34_epoch1.pth\n","  --> New best model saved (val_loss=0.7597)\n","Epoch 2/200 Train Loss: 0.6926 Acc: 0.5454 | Val Loss: 0.7198 Acc: 0.5000 | Time: 106.0s\n","  --> Saved checkpoint to ./checkpoints_simple/resnet34_epoch2.pth\n","  --> New best model saved (val_loss=0.7198)\n","Epoch 3/200 Train Loss: 0.6651 Acc: 0.6028 | Val Loss: 0.7895 Acc: 0.5035 | Time: 104.6s\n","  --> Saved checkpoint to ./checkpoints_simple/resnet34_epoch3.pth\n","Epoch 4/200 Train Loss: 0.6240 Acc: 0.6478 | Val Loss: 0.7445 Acc: 0.4955 | Time: 110.7s\n","  --> Saved checkpoint to ./checkpoints_simple/resnet34_epoch4.pth\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n","File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet34\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","# -----------------------------\n","# 1) Hyperparameters & Setup\n","# -----------------------------\n","EPOCHS = 200\n","EARLY_STOP_PATIENCE = 10\n","LR = 1e-4\n","WEIGHT_DECAY = 1e-5\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","CHECKPOINT_DIR = './checkpoints_simple'\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","\n","# Create a TensorBoard writer\n","writer = SummaryWriter(log_dir='runs/resnet34_simple')\n","\n","# -----------------------------\n","# 2) Model, Loss, Optimizer, Scheduler\n","# -----------------------------\n","model = resnet34(pretrained=False)\n","# ResNet34 default final layer has 1000 outputs; replace with 2 for binary classification\n","model.fc = nn.Linear(model.fc.in_features, 2)\n","model.to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss().to(DEVICE)  # Moved criterion to the same device\n","optimizer = optim.AdamW(model.parameters(), lr=LR)\n","scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-5)\n","\n","# -----------------------------\n","# 3) Helper to compute accuracy\n","# -----------------------------\n","def compute_accuracy(outputs, labels):\n","    \"\"\"\n","    outputs: (batch_size, 2) raw logits\n","    labels: (batch_size,) ground truth in {0,1}\n","    returns accuracy in [0..1].\n","    \"\"\"\n","    preds = outputs.argmax(dim=1)\n","    correct = (preds == labels).sum().item()\n","    total = labels.size(0)\n","    return correct / total\n","\n","# -----------------------------\n","# 4) Training & Validation Loop\n","# -----------------------------\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","for epoch in range(1, EPOCHS + 1):\n","    start_time = time.time()\n","\n","    # ========== TRAIN ==========\n","    model.train()\n","    train_loss, train_correct, train_samples = 0.0, 0, 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Forward\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Metrics\n","        batch_size = labels.size(0)\n","        train_loss += loss.item() * batch_size\n","        train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","        train_samples += batch_size\n","\n","    avg_train_loss = train_loss / train_samples\n","    avg_train_acc  = train_correct / train_samples\n","\n","    # ========== VALIDATION ==========\n","    model.eval()\n","    val_loss, val_correct, val_samples = 0.0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            batch_size = labels.size(0)\n","            val_loss += loss.item() * batch_size\n","            val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","            val_samples += batch_size\n","\n","    avg_val_loss = val_loss / val_samples\n","    avg_val_acc  = val_correct / val_samples\n","\n","    # ========== LOGGING ==========\n","    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n","    writer.add_scalar('Loss/Val',   avg_val_loss,   epoch)\n","    writer.add_scalar('Acc/Train',  avg_train_acc,  epoch)\n","    writer.add_scalar('Acc/Val',    avg_val_acc,    epoch)\n","\n","    elapsed = time.time() - start_time\n","    print(f\"Epoch {epoch}/{EPOCHS} \"\n","          f\"Train Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.4f} | \"\n","          f\"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.4f} | \"\n","          f\"Time: {elapsed:.1f}s\")\n","\n","    # ========== CHECKPOINT (Saving More Info) ==========\n","    checkpoint_dict = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'best_val_loss': best_val_loss\n","    }\n","    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"resnet34_epoch{epoch}.pth\")\n","    torch.save(checkpoint_dict, checkpoint_path)\n","    print(f\"  --> Saved checkpoint to {checkpoint_path}\")\n","\n","    # ========== EARLY STOPPING + SAVE BEST MODEL ==========\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0\n","\n","        # Save the best model so far (same extra info)\n","        best_checkpoint_dict = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'best_val_loss': best_val_loss\n","        }\n","        torch.save(best_checkpoint_dict, os.path.join(CHECKPOINT_DIR, \"best_model.pth\"))\n","        print(f\"  --> New best model saved (val_loss={avg_val_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","\n","    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n","        print(f\"Early stopping at epoch {epoch}. Best val_loss={best_val_loss:.4f}\")\n","        break\n","\n","    # Step the scheduler at the end of each epoch\n","    scheduler.step()\n","\n","# End of training\n","writer.close()\n","print(\"Training complete!\")"]},{"cell_type":"code","execution_count":null,"id":"c8aefd7f-47a4-4071-b652-0551b20dfe18","metadata":{"id":"c8aefd7f-47a4-4071-b652-0551b20dfe18"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0ed132eb-b04e-4a51-a46f-0d279daf646c","metadata":{"id":"0ed132eb-b04e-4a51-a46f-0d279daf646c"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"5a34c076-1450-4487-bdff-16641455c844","metadata":{"id":"5a34c076-1450-4487-bdff-16641455c844"},"source":["After your code finishes (or even while it’s still training), open a terminal (or a separate notebook cell) and run:\n","tensorboard --logdir=Downloads/runs/resnet34_simple\n","Then, in your web browser, go to:\n","http://localhost:6006\n","You’ll see the TensorBoard UI, where you can view the train/val loss and train/val accuracy curves under the “Scalars” tab."]},{"cell_type":"markdown","id":"0f1065b5-64e0-46c9-a543-9eb809d1ce3b","metadata":{"id":"0f1065b5-64e0-46c9-a543-9eb809d1ce3b"},"source":["Loading a Checkpoint to Resume Training"]},{"cell_type":"code","execution_count":null,"id":"e1adace6-76ee-46b8-a853-145bdafd7514","metadata":{"id":"e1adace6-76ee-46b8-a853-145bdafd7514","outputId":"e085c67d-e7f7-44fd-fc1e-e29da00aac4d"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-03-25 16:19:35.249203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742919575.921325    4173 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742919576.248892    4173 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-25 16:19:39.067784: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/home/rayzzz0403/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/rayzzz0403/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Resuming training from epoch 7, best_val_loss so far: 0.7198\n","Epoch 7/10 | Train Loss: 0.2856, Acc: 0.8812 | Val Loss: 1.5615, Acc: 0.4950 | Time: 132.4s\n","  --> Saved checkpoint: ./checkpoints_simple/resnet34_epoch7.pth\n","Epoch 8/10 | Train Loss: 0.1934, Acc: 0.9204 | Val Loss: 1.5751, Acc: 0.4940 | Time: 114.0s\n","  --> Saved checkpoint: ./checkpoints_simple/resnet34_epoch8.pth\n","Epoch 9/10 | Train Loss: 0.1288, Acc: 0.9502 | Val Loss: 2.0548, Acc: 0.5005 | Time: 123.5s\n","  --> Saved checkpoint: ./checkpoints_simple/resnet34_epoch9.pth\n","Epoch 10/10 | Train Loss: 0.0557, Acc: 0.9798 | Val Loss: 2.0109, Acc: 0.4835 | Time: 119.8s\n","  --> Saved checkpoint: ./checkpoints_simple/resnet34_epoch10.pth\n","Resumed training complete!\n","TensorBoard writer closed.\n"]}],"source":["import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet34\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# 1) Decide which device to use\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","CHECKPOINT_DIR = './checkpoints_simple'\n","\n","# 2) Load the checkpoint from disk\n","checkpoint_path = os.path.join(CHECKPOINT_DIR, 'resnet34_epoch6.pth')\n","checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n","\n","# 3) Rebuild the same model architecture\n","model_resumed = resnet34(pretrained=False)\n","model_resumed.fc = nn.Linear(model_resumed.fc.in_features, 2)  # 2-class final layer\n","model_resumed.to(DEVICE)\n","\n","# 4) Rebuild optimizer & schedule\n","criterion_resumed = nn.CrossEntropyLoss().to(DEVICE)\n","optimizer_resumed = optim.AdamW(model_resumed.parameters(), lr=1e-4)\n","scheduler_resumed = CosineAnnealingLR(optimizer_resumed, T_max=15, eta_min=1e-5)\n","\n","# 5) Load the saved states\n","model_resumed.load_state_dict(checkpoint['model_state_dict'])\n","optimizer_resumed.load_state_dict(checkpoint['optimizer_state_dict'])\n","scheduler_resumed.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","start_epoch = checkpoint['epoch'] + 1\n","best_val_loss = checkpoint['best_val_loss']\n","\n","print(f\"Resuming training from epoch {start_epoch}, best_val_loss so far: {best_val_loss:.4f}\")\n","\n","# 6) Set up TensorBoard writer\n","# You can change 'runs/resnet34_resume' to any folder you like\n","writer = SummaryWriter(log_dir='runs/resnet34_resume')\n","\n","# 7) Continue training loop\n","EPOCHS = 10  # or however many total epochs you want to run now\n","\n","for epoch in range(start_epoch, EPOCHS + 1):\n","    start_time = time.time()\n","\n","    # ---------- TRAIN ----------\n","    model_resumed.train()\n","    train_loss, train_correct, train_samples = 0.0, 0, 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        outputs = model_resumed(images)\n","        loss = criterion_resumed(outputs, labels)\n","\n","        optimizer_resumed.zero_grad()\n","        loss.backward()\n","        optimizer_resumed.step()\n","\n","        batch_size = labels.size(0)\n","        train_loss += loss.item() * batch_size\n","        train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","        train_samples += batch_size\n","\n","    avg_train_loss = train_loss / train_samples\n","    avg_train_acc  = train_correct / train_samples\n","\n","    # ---------- VALID ----------\n","    model_resumed.eval()\n","    val_loss, val_correct, val_samples = 0.0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","            outputs = model_resumed(images)\n","            loss = criterion_resumed(outputs, labels)\n","\n","            batch_size = labels.size(0)\n","            val_loss += loss.item() * batch_size\n","            val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","            val_samples += batch_size\n","\n","    avg_val_loss = val_loss / val_samples\n","    avg_val_acc  = val_correct / val_samples\n","\n","    elapsed = time.time() - start_time\n","    print(f\"Epoch {epoch}/{EPOCHS} | \"\n","          f\"Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.4f} | \"\n","          f\"Val Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.4f} | \"\n","          f\"Time: {elapsed:.1f}s\")\n","\n","    # ---------- TensorBoard Logging ----------\n","    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n","    writer.add_scalar('Loss/Val',   avg_val_loss,   epoch)\n","    writer.add_scalar('Acc/Train',  avg_train_acc,  epoch)\n","    writer.add_scalar('Acc/Val',    avg_val_acc,    epoch)\n","\n","    # ---------- Save a checkpoint each epoch ----------\n","    checkpoint_dict = {\n","        'epoch': epoch,\n","        'model_state_dict': model_resumed.state_dict(),\n","        'optimizer_state_dict': optimizer_resumed.state_dict(),\n","        'scheduler_state_dict': scheduler_resumed.state_dict(),\n","        'best_val_loss': best_val_loss\n","    }\n","\n","    epoch_ckpt_path = os.path.join(CHECKPOINT_DIR, f\"resnet34_epoch{epoch}.pth\")\n","    torch.save(checkpoint_dict, epoch_ckpt_path)\n","    print(f\"  --> Saved checkpoint: {epoch_ckpt_path}\")\n","\n","    # ---------- If improved, save best model ----------\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        print(f\"  --> New best val_loss: {best_val_loss:.4f}\")\n","        torch.save(checkpoint_dict, os.path.join(CHECKPOINT_DIR, \"best_model.pth\"))\n","        print(f\"  --> Saved best model: {os.path.join(CHECKPOINT_DIR, 'best_model.pth')}\")\n","\n","    # ---------- Step scheduler ----------\n","    scheduler_resumed.step()\n","\n","print(\"Resumed training complete!\")\n","writer.close()\n","print(\"TensorBoard writer closed.\")"]},{"cell_type":"code","execution_count":null,"id":"595a5c98-7fe0-44bb-834d-611660a2f47b","metadata":{"id":"595a5c98-7fe0-44bb-834d-611660a2f47b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}